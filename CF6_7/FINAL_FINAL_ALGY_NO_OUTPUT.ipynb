{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhymlGLWDJJd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import os, glob, random\n",
        "# from IPython.display import Audio, display # Keep this if in Colab/Jupyter\n",
        "\n",
        "# --- Constants ---\n",
        "TARGET_SR = 44100\n",
        "N_FFT = 1024 # Window size for STFT\n",
        "HOP_LENGTH = 256 # Hop length for STFT\n",
        "N_MELS = 128 # Number of Mel frequency bins\n",
        "LATENT_DIM = 64\n",
        "FIXED_FRAMES = 512 # Target number of frames in the spectrogram\n",
        "TOTAL_EPOCHS = 3000\n",
        "CHUNK_SIZE = 100\n",
        "NUM_CHUNKS = TOTAL_EPOCHS // CHUNK_SIZE\n",
        "KL_TARGET = 0.1\n",
        "KL_WARMUP = 500\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "folder_path = '/content/drive/MyDrive/Neural Drum Machine/Samples/01. Bass Drum' # UPDATE THIS\n",
        "\n",
        "if not os.path.exists(folder_path) or not os.listdir(folder_path):\n",
        "    print(f\"Warning: Folder '{folder_path}' is empty or not found.\")\n",
        "    print(\"Attempting to use/create dummy data.\")\n",
        "    dummy_data_dir = \"dummy_audio_data_mel\"\n",
        "    os.makedirs(dummy_data_dir, exist_ok=True)\n",
        "    dummy_file_path = os.path.join(dummy_data_dir, \"dummy_kick_mel.wav\")\n",
        "    if not os.path.exists(dummy_file_path):\n",
        "        try:\n",
        "            from scipy.io.wavfile import write as write_wav\n",
        "            sample_rate = TARGET_SR; duration = 1; frequency = 440\n",
        "            t = np.linspace(0, duration, int(sample_rate * duration), False)\n",
        "            note = np.sin(frequency * t * 2 * np.pi)\n",
        "            audio_data = (note * (32767 / np.max(np.abs(note)))).astype(np.int16)\n",
        "            write_wav(dummy_file_path, sample_rate, audio_data)\n",
        "            print(f\"Created dummy WAV: {dummy_file_path}\")\n",
        "        except Exception as e: print(f\"Error creating dummy WAV: {e}\")\n",
        "    if os.path.exists(dummy_file_path): folder_path = dummy_data_dir\n",
        "    else: print(\"Could not create/find dummy audio.\")\n",
        "\n",
        "files_list = glob.glob(os.path.join(folder_path, '*.wav'))\n",
        "print(f\"Found {len(files_list)} samples in '{folder_path}'.\")\n",
        "\n",
        "def wav_to_mel_spec(filename, sr=TARGET_SR, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS, target_frames=FIXED_FRAMES):\n",
        "    \"\"\"Converts a WAV file to a normalized log-Mel spectrogram.\"\"\"\n",
        "    try:\n",
        "        y, file_sr = librosa.load(filename, sr=None)\n",
        "        if file_sr != sr: y = librosa.resample(y, orig_sr=file_sr, target_sr=sr)\n",
        "\n",
        "        y, _ = librosa.effects.trim(y, top_db=30)\n",
        "\n",
        "        target_length = sr\n",
        "        if len(y) > target_length: y = y[:target_length]\n",
        "        else: y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
        "\n",
        "        # Compute Mel spectrogram\n",
        "        S_mel = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
        "\n",
        "        # Convert to log scale (log(1 + S_mel) for stability)\n",
        "        log_S_mel = np.log1p(S_mel)\n",
        "\n",
        "        # Normalize\n",
        "        if log_S_mel.max() > 0:\n",
        "            log_S_mel = log_S_mel / log_S_mel.max()\n",
        "        else:\n",
        "            log_S_mel = np.zeros_like(log_S_mel)\n",
        "\n",
        "        # Pad or truncate to FIXED_FRAMES\n",
        "        if log_S_mel.shape[1] < target_frames:\n",
        "            pad_width = target_frames - log_S_mel.shape[1]\n",
        "            log_S_mel = np.pad(log_S_mel, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
        "        else:\n",
        "            log_S_mel = log_S_mel[:, :target_frames]\n",
        "\n",
        "        return log_S_mel\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {filename} for Mel spec: {e}\")\n",
        "        return np.zeros((n_mels, target_frames), dtype=np.float32)\n",
        "\n",
        "SAMPLES_LIST = [wav_to_mel_spec(f) for f in files_list if os.path.exists(f)]\n",
        "SAMPLES_LIST = [s for s in SAMPLES_LIST if s is not None and s.shape == (N_MELS, FIXED_FRAMES)]\n",
        "\n",
        "if not SAMPLES_LIST:\n",
        "    print(f\"Could not load valid Mel spectrograms. Using random noise ({N_MELS}x{FIXED_FRAMES}).\")\n",
        "    SAMPLES = np.random.rand(1, N_MELS, FIXED_FRAMES).astype(np.float32)\n",
        "else:\n",
        "    SAMPLES = np.stack(SAMPLES_LIST)\n",
        "\n",
        "print(f\"SAMPLES (Mel Spectrograms) shape: {SAMPLES.shape}\") # Expected: (N, N_MELS, FIXED_FRAMES)\n",
        "\n",
        "class DrumDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, specs): self.specs = torch.tensor(specs, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.specs)\n",
        "    def __getitem__(self, idx): return self.specs[idx]\n",
        "\n",
        "if SAMPLES.shape[0] > 0:\n",
        "    loader = torch.utils.data.DataLoader(DrumDataset(SAMPLES), batch_size=16, shuffle=True)\n",
        "else:\n",
        "    print(\"No samples for DataLoader.\")\n",
        "    loader = None\n",
        "\n",
        "# --- Simple VAE (Encoder and Decoder adjusted for N_MELS) ---\n",
        "class SimpleEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM, n_input_channels=N_MELS): # n_input_channels is N_MELS\n",
        "        super().__init__()\n",
        "        # Input: (B, N_MELS, FIXED_FRAMES) e.g. (B, 128, 512)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(n_input_channels, 128, kernel_size=4, stride=2, padding=1), # (B, 128, 256)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 64, kernel_size=4, stride=2, padding=1),            # (B, 64, 128)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64, 32, kernel_size=4, stride=2, padding=1),             # (B, 32, 64)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.final_conv_output_frames = FIXED_FRAMES // 8\n",
        "        self.mu = nn.Linear(32 * self.final_conv_output_frames, latent_dim)\n",
        "        self.logvar = nn.Linear(32 * self.final_conv_output_frames, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        h = self.flatten(h)\n",
        "        return self.mu(h), self.logvar(h)\n",
        "\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM, n_output_channels=N_MELS): # n_output_channels is N_MELS\n",
        "        super().__init__()\n",
        "        self.n_output_channels = n_output_channels\n",
        "        self.initial_frames = FIXED_FRAMES // 4\n",
        "        self.initial_channels = 64\n",
        "\n",
        "        self.fc = nn.Linear(latent_dim, self.initial_channels * self.initial_frames)\n",
        "\n",
        "        self.decode_layers = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv1d(self.initial_channels, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv1d(32, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(16, self.n_output_channels, kernel_size=1), # Output N_MELS channels\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.fc(z)\n",
        "        x = x.view(-1, self.initial_channels, self.initial_frames)\n",
        "        recon_mel_spec = self.decode_layers(x)\n",
        "        return recon_mel_spec\n",
        "\n",
        "class SimpleVAE(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.encoder = SimpleEncoder(latent_dim, n_input_channels=N_MELS)\n",
        "        self.decoder = SimpleDecoder(latent_dim, n_output_channels=N_MELS)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encoder(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon_x = self.decoder(z)\n",
        "        return recon_x, mu, logvar\n",
        "\n",
        "# --- Setup for Training ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "vae = SimpleVAE(latent_dim=LATENT_DIM).to(device)\n",
        "opt = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "def kl_divergence_loss(mu, logvar):\n",
        "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
        "\n",
        "def mel_spec_to_audio(log_mel_spec_norm, sr=TARGET_SR, n_fft=N_FFT, hop_length=HOP_LENGTH):\n",
        "    \"\"\"Converts a normalized log-Mel spectrogram back to audio.\"\"\"\n",
        "    if isinstance(log_mel_spec_norm, torch.Tensor):\n",
        "        log_mel_spec_norm = log_mel_spec_norm.detach().cpu().numpy()\n",
        "\n",
        "    current_max = log_mel_spec_norm.max()\n",
        "    if current_max <= 1e-6: # Effectively silent\n",
        "        return np.zeros(sr)\n",
        "\n",
        "    # Denormalize based on its own max (consistent with input processing)\n",
        "    log_mel_spec_scaled = log_mel_spec_norm * current_max\n",
        "\n",
        "    # Inverse of log1p is expm1\n",
        "    mel_spec_approx = np.expm1(log_mel_spec_scaled)\n",
        "\n",
        "    # Convert Mel spectrogram to linear magnitude STFT spectrogram\n",
        "    # This is an approximation\n",
        "    stft_magnitude_approx = librosa.feature.inverse.mel_to_stft(\n",
        "        M=mel_spec_approx, sr=sr, n_fft=n_fft\n",
        "    )\n",
        "\n",
        "    # Griffin-Lim algorithm to reconstruct phase and convert to audio\n",
        "    audio_out = librosa.griffinlim(stft_magnitude_approx, n_iter=32, hop_length=hop_length, n_fft=n_fft)\n",
        "    return audio_out\n",
        "\n",
        "# --- Training Loop ---\n",
        "if loader is not None and SAMPLES.shape[0] > 0:\n",
        "    start_epoch = 0\n",
        "    for chunk_idx in range(NUM_CHUNKS):\n",
        "        print(f\"\\n--- Chunk {chunk_idx+1}/{NUM_CHUNKS} (Epochs {start_epoch+1}-{start_epoch+CHUNK_SIZE}) ---\")\n",
        "        for epoch in range(start_epoch, start_epoch + CHUNK_SIZE):\n",
        "            vae.train()\n",
        "            total_recon_loss_epoch, total_kl_loss_epoch = 0, 0\n",
        "            kl_weight = min(KL_TARGET, KL_TARGET * (epoch / KL_WARMUP) if KL_WARMUP > 0 else KL_TARGET)\n",
        "\n",
        "            for batch_specs in loader:\n",
        "                batch_specs = batch_specs.to(device)\n",
        "                opt.zero_grad()\n",
        "                recon_specs, mu, logvar = vae(batch_specs)\n",
        "\n",
        "                recon_loss = F.l1_loss(recon_specs, batch_specs, reduction='sum') / batch_specs.size(0)\n",
        "                kl_loss_batch = kl_divergence_loss(mu, logvar).mean()\n",
        "                loss = recon_loss + kl_weight * kl_loss_batch\n",
        "\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "                total_recon_loss_epoch += recon_loss.item() * batch_specs.size(0)\n",
        "                total_kl_loss_epoch += kl_loss_batch.item() * batch_specs.size(0)\n",
        "\n",
        "            avg_recon_loss = total_recon_loss_epoch / len(SAMPLES)\n",
        "            avg_kl_loss = total_kl_loss_epoch / len(SAMPLES)\n",
        "            avg_total_loss = avg_recon_loss + kl_weight * avg_kl_loss\n",
        "\n",
        "            if (epoch + 1) % 50 == 0 or epoch == start_epoch + CHUNK_SIZE -1:\n",
        "                print(f\"Epoch {epoch+1:03d} | Total Loss: {avg_total_loss:.4f} | Recon: {avg_recon_loss:.4f} | KL: {avg_kl_loss:.4f} (W: {kl_weight:.4f})\")\n",
        "\n",
        "        # --- Preview ---\n",
        "        vae.eval()\n",
        "        with torch.no_grad():\n",
        "            preview_batch = next(iter(loader)).to(device)\n",
        "            idx = random.randint(0, preview_batch.size(0) - 1)\n",
        "            real_spec_tensor = preview_batch[idx:idx+1]\n",
        "            recon_spec_tensor, _, _ = vae(real_spec_tensor)\n",
        "            random_latent_z = torch.randn(1, LATENT_DIM).to(device)\n",
        "            generated_spec_tensor = vae.decoder(random_latent_z)\n",
        "\n",
        "            real_np = real_spec_tensor.squeeze().cpu().numpy()\n",
        "            recon_np = recon_spec_tensor.squeeze().cpu().numpy()\n",
        "            generated_np = generated_spec_tensor.squeeze().cpu().numpy()\n",
        "\n",
        "            plt.figure(figsize=(18, 5))\n",
        "            plt.suptitle(f\"VAE (Mel Spec Decoder) - Chunk {chunk_idx+1} â€” Epoch {start_epoch+CHUNK_SIZE}\", fontsize=16)\n",
        "\n",
        "            plt.subplot(1, 3, 1); librosa.display.specshow(real_np, sr=TARGET_SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel'); plt.title(\"Original Mel Spec\"); plt.colorbar(format='%+2.0f dB')\n",
        "            plt.subplot(1, 3, 2); librosa.display.specshow(recon_np, sr=TARGET_SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel'); plt.title(\"Reconstructed Mel Spec\"); plt.colorbar(format='%+2.0f dB')\n",
        "            plt.subplot(1, 3, 3); librosa.display.specshow(generated_np, sr=TARGET_SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel'); plt.title(\"Generated Mel Spec\"); plt.colorbar(format='%+2.0f dB')\n",
        "\n",
        "            plt.tight_layout(rect=[0, 0.05, 1, 0.95]); plt.show()\n",
        "\n",
        "            try:\n",
        "                from IPython.display import Audio, display\n",
        "                print(\"\\nOriginal Kick (from Mel Spec):\"); display(Audio(mel_spec_to_audio(real_np), rate=TARGET_SR, normalize=False))\n",
        "                print(\"Reconstructed Kick (from Mel Spec):\"); display(Audio(mel_spec_to_audio(recon_np), rate=TARGET_SR, normalize=False))\n",
        "                print(\"Generated Kick (from Mel Spec):\"); display(Audio(mel_spec_to_audio(generated_np), rate=TARGET_SR, normalize=False))\n",
        "            except ImportError: print(\"\\nIPython.display.Audio not available. Skipping audio playback.\")\n",
        "        start_epoch += CHUNK_SIZE\n",
        "else:\n",
        "    print(\"Skipping training: No data or loader.\")\n",
        "print(\"\\n--- Script Finished ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# --- Constants (ensure these match your VAE's decoder architecture) ---\n",
        "LATENT_DIM = 64\n",
        "N_MELS = 128       # Number of Mel bins the decoder outputs\n",
        "FIXED_FRAMES = 512 # Number of time frames the decoder outputs\n",
        "\n",
        "# --- Simplified Decoder Definition (from your VAE for Mel Spectrograms) ---\n",
        "class SimpleDecoder(nn.Module):\n",
        "    \"\"\"Simplified Decoder part of the VAE using Upsample + Conv1D.\"\"\"\n",
        "    def __init__(self, latent_dim=LATENT_DIM, n_output_channels=N_MELS, fixed_frames=FIXED_FRAMES):\n",
        "        super().__init__()\n",
        "        self.n_output_channels = n_output_channels\n",
        "        # Initial number of frames and channels after the FC layer, before upsampling\n",
        "        self.initial_frames = fixed_frames // 4  # e.g., 512 / 4 = 128. We'll upsample twice by 2x.\n",
        "        self.initial_channels = 64  # Hyperparameter: can be tuned\n",
        "\n",
        "        # Fully connected layer to project latent space to initial deconv shape\n",
        "        self.fc = nn.Linear(latent_dim, self.initial_channels * self.initial_frames)\n",
        "\n",
        "        # Upsampling and convolutional layers\n",
        "        self.decode_layers = nn.Sequential(\n",
        "            nn.ReLU(), # Activation after FC\n",
        "\n",
        "            # Block 1: Upsample from initial_frames to initial_frames*2\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'), # (B, initial_channels, initial_frames*2)\n",
        "            nn.Conv1d(self.initial_channels, 32, kernel_size=3, stride=1, padding=1), # (B, 32, initial_frames*2)\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Block 2: Upsample from initial_frames*2 to initial_frames*4 (target fixed_frames)\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'), # (B, 32, fixed_frames)\n",
        "            nn.Conv1d(32, 16, kernel_size=3, stride=1, padding=1), # (B, 16, fixed_frames)\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Final convolution to map to the target number of output channels (n_output_channels)\n",
        "            nn.Conv1d(16, self.n_output_channels, kernel_size=1), # (B, n_output_channels, fixed_frames)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, z): # z: (B, latent_dim)\n",
        "        # Project latent vector and reshape to start decoding\n",
        "        x = self.fc(z) # (B, initial_channels * initial_frames)\n",
        "        x = x.view(-1, self.initial_channels, self.initial_frames) # (B, initial_channels, initial_frames)\n",
        "\n",
        "        # Pass through upsampling and convolutional layers\n",
        "        recon_spec = self.decode_layers(x) # (B, n_output_channels, fixed_frames)\n",
        "        return recon_spec\n",
        "\n",
        "def test_decoder_inference_speed(decoder_model, latent_dimension, current_device, num_runs=100, warm_up_runs=10):\n",
        "    \"\"\"\n",
        "    Tests the inference speed of the provided decoder model.\n",
        "\n",
        "    Args:\n",
        "        decoder_model (nn.Module): The decoder model instance.\n",
        "        latent_dimension (int): The dimension of the latent space.\n",
        "        current_device (torch.device): The device to run the model on (e.g., 'cuda' or 'cpu').\n",
        "        num_runs (int): Number of inference runs to average for timing.\n",
        "        warm_up_runs (int): Number of initial runs to discard.\n",
        "    \"\"\"\n",
        "    decoder_model.eval() # Set the model to evaluation mode\n",
        "    decoder_model.to(current_device) # Ensure model is on the correct device\n",
        "\n",
        "    inference_times = []\n",
        "\n",
        "    print(f\"\\n--- Starting Decoder Inference Speed Test ---\")\n",
        "    print(f\"Device: {current_device}\")\n",
        "    print(f\"Input latent dimension: {latent_dimension}\")\n",
        "    print(f\"Output spectrogram shape (expected): (1, {decoder_model.n_output_channels}, {FIXED_FRAMES})\") # Assuming fixed_frames is global or passed\n",
        "    print(f\"Warm-up runs: {warm_up_runs}\")\n",
        "    print(f\"Timed runs: {num_runs}\")\n",
        "\n",
        "    # Create a dummy input tensor once\n",
        "    # Batch size of 1 for single latent vector test\n",
        "    z_input = torch.randn(1, latent_dimension).to(current_device)\n",
        "\n",
        "    for i in range(warm_up_runs + num_runs):\n",
        "        if current_device.type == 'cuda':\n",
        "            torch.cuda.synchronize() # Wait for all preceding GPU ops to finish\n",
        "\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient calculations for inference\n",
        "            _ = decoder_model(z_input) # Perform inference\n",
        "\n",
        "        if current_device.type == 'cuda':\n",
        "            torch.cuda.synchronize() # Wait for decoder op to finish\n",
        "\n",
        "        end_time = time.perf_counter()\n",
        "\n",
        "        if i >= warm_up_runs: # Only record times after warm-up\n",
        "            time_ms = (end_time - start_time) * 1000\n",
        "            inference_times.append(time_ms)\n",
        "\n",
        "    if not inference_times:\n",
        "        print(\"No timed runs were recorded. Check num_runs.\")\n",
        "        return\n",
        "\n",
        "    # --- Calculate and print statistics ---\n",
        "    avg_time = np.mean(inference_times)\n",
        "    std_time = np.std(inference_times)\n",
        "    min_time = np.min(inference_times)\n",
        "    max_time = np.max(inference_times)\n",
        "\n",
        "    print(\"\\n--- Decoder Speed Test Results ---\")\n",
        "    print(f\"Average Inference Time (per latent vector): {avg_time:.4f} ms\")\n",
        "    print(f\"Standard Deviation: {std_time:.4f} ms\")\n",
        "    print(f\"Min Time: {min_time:.4f} ms, Max Time: {max_time:.4f} ms\")\n",
        "\n",
        "    fps = 1000 / avg_time if avg_time > 0 else float('inf')\n",
        "    print(f\"Approximate Throughput: {fps:.2f} latent vectors/second\")\n",
        "    print(\"--- End of Decoder Speed Test ---\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Determine device\n",
        "    selected_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Instantiate the decoder\n",
        "    # Pass fixed_frames to the constructor if it's used internally for layer sizing\n",
        "    decoder = SimpleDecoder(latent_dim=LATENT_DIM,\n",
        "                            n_output_channels=N_MELS,\n",
        "                            fixed_frames=FIXED_FRAMES)\n",
        "\n",
        "    # --- IMPORTANT ---\n",
        "    # For a realistic speed test of a TRAINED model, you would load its state_dict here:\n",
        "    # try:\n",
        "    #     # model_path = 'path_to_your_trained_vae.pth' # Path to the full VAE model\n",
        "    #     # full_vae_state_dict = torch.load(model_path, map_location=selected_device)\n",
        "    #     # decoder_state_dict = {k.replace('decoder.', ''): v for k, v in full_vae_state_dict.items() if k.startswith('decoder.')}\n",
        "    #     # decoder.load_state_dict(decoder_state_dict)\n",
        "    #     # print(f\"Successfully loaded trained decoder weights for speed test.\")\n",
        "    #     print(\"Skipping loading of trained decoder weights for this simple test. Using initialized model.\")\n",
        "    # except FileNotFoundError:\n",
        "    #     print(f\"Warning: Trained model weights file not found. Speed test will run with an UNTRAINED (randomly initialized) decoder.\")\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error loading decoder weights: {e}. Speed test will run with an UNTRAINED decoder.\")\n",
        "    print(\"Using initialized (untrained) decoder for speed test.\")\n",
        "\n",
        "    # Run the speed test\n",
        "    test_decoder_inference_speed(decoder,\n",
        "                                 LATENT_DIM,\n",
        "                                 selected_device,\n",
        "                                 num_runs=200,  # Increase for more stable averages\n",
        "                                 warm_up_runs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hdmlJ6yDWl4",
        "outputId": "8531fba6-174e-47d8-90ad-b247f3ef483a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using initialized (untrained) decoder for speed test.\n",
            "\n",
            "--- Starting Decoder Inference Speed Test ---\n",
            "Device: cuda\n",
            "Input latent dimension: 64\n",
            "Output spectrogram shape (expected): (1, 128, 512)\n",
            "Warm-up runs: 20\n",
            "Timed runs: 200\n",
            "\n",
            "--- Decoder Speed Test Results ---\n",
            "Average Inference Time (per latent vector): 0.4897 ms\n",
            "Standard Deviation: 0.0652 ms\n",
            "Min Time: 0.4547 ms, Max Time: 1.2477 ms\n",
            "Approximate Throughput: 2041.97 latent vectors/second\n",
            "--- End of Decoder Speed Test ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os # For loading model\n",
        "\n",
        "# --- Constants (from your VAE decoder) ---\n",
        "LATENT_DIM = 64\n",
        "N_MELS = 128\n",
        "FIXED_FRAMES = 512\n",
        "\n",
        "# --- SimpleDecoder Definition ---\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM, n_output_channels=N_MELS, fixed_frames=FIXED_FRAMES):\n",
        "        super().__init__()\n",
        "        self.n_output_channels = n_output_channels\n",
        "        self.initial_frames = fixed_frames // 4\n",
        "        self.initial_channels = 64\n",
        "\n",
        "        self.fc_out_features = self.initial_channels * self.initial_frames\n",
        "        self.fc = nn.Linear(latent_dim, self.fc_out_features)\n",
        "\n",
        "        # We'll simulate layer by layer. For now, define up to the first ReLU.\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # Placeholder for subsequent layers - we'll add them as we simulate\n",
        "        # self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        # self.conv1 = nn.Conv1d(self.initial_channels, 32, kernel_size=3, stride=1, padding=1)\n",
        "        # ... and so on\n",
        "\n",
        "    def forward_fc_only(self, z):\n",
        "        \"\"\"Forward pass up to the output of the fc layer.\"\"\"\n",
        "        return self.fc(z)\n",
        "\n",
        "    def forward_fc_relu1(self, z):\n",
        "        \"\"\"Forward pass up to the output of the first ReLU.\"\"\"\n",
        "        x_fc = self.fc(z)\n",
        "        x_relu1 = self.relu1(x_fc)\n",
        "        return x_fc, x_relu1 # Return intermediate for analysis if needed\n",
        "\n",
        "    # Full forward pass (for completeness, not fully simulated in fixed-point yet)\n",
        "    def forward(self, z):\n",
        "        x = self.fc(z)\n",
        "        x = self.relu1(x) # First ReLU\n",
        "\n",
        "        # Reshape before convolutions\n",
        "        x_reshaped = x.view(-1, self.initial_channels, self.initial_frames)\n",
        "\n",
        "        # The rest of the decode_layers (conceptual, from original model)\n",
        "        # x_upsample1 = self.upsample1(x_reshaped)\n",
        "        # x_conv1 = self.conv1(x_upsample1)\n",
        "        # ...\n",
        "        # For now, this function is not fully implemented with all layers defined above.\n",
        "        # We are focusing on fc and relu1 first.\n",
        "        # To make this runnable as-is, we'd need to define all layers or return early.\n",
        "        # For this script's purpose, we'll call specific forward_fc_relu1 etc.\n",
        "\n",
        "        # This is just a placeholder to make the class complete\n",
        "        # In a full fixed-point sim, you'd pass data through each simulated layer.\n",
        "        # For now, let's assume we are interested in the output of relu1 reshaped.\n",
        "        return x.view(-1, self.initial_channels, self.initial_frames) # Example output shape\n",
        "\n",
        "\n",
        "# --- Fixed-Point Simulation Parameters ---\n",
        "# YOU MUST DETERMINE THESE BASED ON YOUR RANGE ANALYSIS OF THE TRAINED MODEL\n",
        "TOTAL_BITS = 16  # Example: 16-bit fixed-point for data and weights\n",
        "FRAC_BITS = 8   # Example: 8 fractional bits for data and weights\n",
        "# This implies: Integer Bits = TOTAL_BITS - FRAC_BITS - 1 (for sign) = 7\n",
        "\n",
        "# Parameters for weights (could be different from activations)\n",
        "WEIGHT_TOTAL_BITS = 16\n",
        "WEIGHT_FRAC_BITS = 10 # Example: more fractional bits for weights if their range is small (e.g. [-2, 2])\n",
        "\n",
        "# --- Quantization and Dequantization Functions ---\n",
        "def quantize_value(value, total_bits, frac_bits):\n",
        "    scale = 2.0**frac_bits\n",
        "    min_val_scaled = -(2.0**(total_bits - 1))\n",
        "    max_val_scaled = (2.0**(total_bits - 1)) - 1\n",
        "    scaled_value = np.round(value * scale)\n",
        "    clamped_value = np.clip(scaled_value, min_val_scaled, max_val_scaled)\n",
        "    return clamped_value.astype(np.int64)\n",
        "\n",
        "def dequantize_value(quantized_value_scaled, frac_bits):\n",
        "    scale = 2.0**frac_bits\n",
        "    return quantized_value_scaled / scale\n",
        "\n",
        "# --- Main Fixed-Point Analysis Script ---\n",
        "if __name__ == '__main__':\n",
        "    print(f\"--- Fixed-Point Conversion Analysis for ASIC PoC ---\")\n",
        "    print(f\"Activation fixed-point: TOTAL_BITS={TOTAL_BITS}, FRAC_BITS={FRAC_BITS}\")\n",
        "    print(f\"Weight fixed-point: TOTAL_BITS={WEIGHT_TOTAL_BITS}, FRAC_BITS={WEIGHT_FRAC_BITS}\\n\")\n",
        "\n",
        "    # 1. Instantiate the decoder\n",
        "    decoder = SimpleDecoder(latent_dim=LATENT_DIM, n_output_channels=N_MELS, fixed_frames=FIXED_FRAMES)\n",
        "\n",
        "    # --- !!! IMPORTANT: Load your TRAINED model weights here !!! ---\n",
        "    # Example:\n",
        "    # model_path = 'path_to_your_trained_SimpleVAE_model.pth'\n",
        "    # if os.path.exists(model_path):\n",
        "    #     try:\n",
        "    #         full_vae_state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "    #         # Filter for decoder weights if VAE model is saved\n",
        "    #         decoder_state_dict = {k.replace('decoder.', '', 1): v\n",
        "    #                               for k, v in full_vae_state_dict.items()\n",
        "    #                               if k.startswith('decoder.')}\n",
        "    #         if not decoder_state_dict: # If only decoder was saved\n",
        "    #             decoder_state_dict = full_vae_state_dict\n",
        "    #         decoder.load_state_dict(decoder_state_dict)\n",
        "    #         print(f\"Successfully loaded trained decoder weights from {model_path}\")\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"Error loading trained weights from {model_path}: {e}\")\n",
        "    #         print(\"Proceeding with randomly initialized weights.\")\n",
        "    # else:\n",
        "    #     print(f\"Trained model weights not found at {model_path}. Proceeding with randomly initialized weights.\")\n",
        "    print(\"NOTE: Ensure you load your TRAINED model weights for accurate analysis.\")\n",
        "    decoder.eval()\n",
        "\n",
        "    # --- 2. Range Analysis (Methodology) ---\n",
        "    print(\"\\n--- Range Analysis Methodology (Action Required) ---\")\n",
        "    print(\"1. Generate a diverse set of N (e.g., 1000-10000) random latent vectors 'z'.\")\n",
        "    print(\"2. For each 'z', perform a full forward pass through your TRAINED FLOATING-POINT decoder.\")\n",
        "    print(\"3. Record the MIN and MAX floating-point values observed for:\")\n",
        "    print(\"   - All weight tensors (e.g., decoder.fc.weight, decoder.conv1.weight, etc.)\")\n",
        "    print(\"   - All bias tensors.\")\n",
        "    print(\"   - The INPUT to EACH layer (activations).\")\n",
        "    print(\"   - The OUTPUT of EACH layer (activations).\")\n",
        "    print(\"4. Use these global MIN/MAX ranges to choose appropriate TOTAL_BITS and FRAC_BITS for weights and activations.\")\n",
        "    print(\"   (You might use different bit-widths for weights vs. activations, or even per layer).\")\n",
        "    print(\"-----------------------------------------------------\\n\")\n",
        "\n",
        "    # --- 3. Extract and Quantize Weights for self.fc ---\n",
        "    fc_weights_float = decoder.fc.weight.data.detach().clone().numpy()\n",
        "    fc_bias_float = decoder.fc.bias.data.detach().clone().numpy()\n",
        "\n",
        "    fc_weights_quant = quantize_value(fc_weights_float, WEIGHT_TOTAL_BITS, WEIGHT_FRAC_BITS)\n",
        "    fc_bias_quant = quantize_value(fc_bias_float, TOTAL_BITS, FRAC_BITS) # Bias often uses activation format\n",
        "\n",
        "    print(f\"FC Layer: fc_weights_quant shape: {fc_weights_quant.shape}, dtype: {fc_weights_quant.dtype}\")\n",
        "    print(f\"FC Layer: fc_bias_quant shape: {fc_bias_quant.shape}, dtype: {fc_bias_quant.dtype}\")\n",
        "\n",
        "\n",
        "    # --- 4. Simulate FC Layer and first ReLU with a sample input ---\n",
        "    sample_z_float_tensor = torch.randn(1, LATENT_DIM) # Batch size 1\n",
        "    sample_z_float_np = sample_z_float_tensor.numpy().squeeze() # (LATENT_DIM,)\n",
        "\n",
        "    # --- Floating-point computation (Reference) ---\n",
        "    fc_output_float_tensor, relu1_output_float_tensor = decoder.forward_fc_relu1(sample_z_float_tensor)\n",
        "    fc_output_float_np = fc_output_float_tensor.detach().numpy().squeeze()\n",
        "    relu1_output_float_np = relu1_output_float_tensor.detach().numpy().squeeze()\n",
        "\n",
        "    print(f\"\\n--- Reference Floating-Point Outputs ---\")\n",
        "    print(f\"FC output float min: {fc_output_float_np.min():.4f}, max: {fc_output_float_np.max():.4f}\")\n",
        "    print(f\"ReLU1 output float min: {relu1_output_float_np.min():.4f}, max: {relu1_output_float_np.max():.4f}\")\n",
        "\n",
        "    # --- Fixed-Point Simulation ---\n",
        "    # Quantize the input sample_z\n",
        "    sample_z_quant = quantize_value(sample_z_float_np, TOTAL_BITS, FRAC_BITS)\n",
        "\n",
        "    # A. Simulate FC Layer (Fixed-Point)\n",
        "    # output_scaled = (input_scaled @ weights_scaled.T) / (2**WEIGHT_FRAC_BITS_FOR_PRODUCT_SCALING) + bias_scaled\n",
        "    # Product: (z_act_q * w_q) -> scale is (2^FRAC_BITS * 2^WEIGHT_FRAC_BITS)\n",
        "    # Accumulator needs to handle this. Then rescale before adding bias.\n",
        "\n",
        "    fc_output_quant_scaled = np.zeros(decoder.fc.out_features, dtype=np.int64)\n",
        "    intermediate_scale_factor = 2.0**WEIGHT_FRAC_BITS # Scale of weights\n",
        "\n",
        "    for i in range(decoder.fc.out_features): # For each output neuron\n",
        "        accumulator = np.int64(0)\n",
        "        for j in range(decoder.fc.in_features): # Dot product\n",
        "            # sample_z_quant[j] is scaled by 2^FRAC_BITS\n",
        "            # fc_weights_quant[i,j] is scaled by 2^WEIGHT_FRAC_BITS\n",
        "            # Product is scaled by 2^(FRAC_BITS + WEIGHT_FRAC_BITS)\n",
        "            product_mega_scaled = np.int64(sample_z_quant[j]) * np.int64(fc_weights_quant[i, j])\n",
        "            accumulator += product_mega_scaled\n",
        "\n",
        "        # Accumulator is sum of (z_q * w_q), scaled by 2^(FRAC_BITS + WEIGHT_FRAC_BITS)\n",
        "        # We want the result to be scaled by 2^FRAC_BITS (like activations)\n",
        "        # So, divide accumulator by 2^WEIGHT_FRAC_BITS\n",
        "        acc_rescaled_for_activation = np.round(accumulator / intermediate_scale_factor).astype(np.int64)\n",
        "        fc_output_quant_scaled[i] = acc_rescaled_for_activation + fc_bias_quant[i] # Bias is already in activation Q format\n",
        "\n",
        "    # Clamp the output of FC layer to activation fixed-point range\n",
        "    fc_output_quant_scaled = np.clip(fc_output_quant_scaled,\n",
        "                                     -(2**(TOTAL_BITS - 1)),\n",
        "                                     (2**(TOTAL_BITS - 1)) - 1)\n",
        "\n",
        "\n",
        "    # B. Simulate ReLU Layer (Fixed-Point)\n",
        "    # ReLU input is fc_output_quant_scaled (which are scaled integers)\n",
        "    # ReLU: max(0, x)\n",
        "    relu1_output_quant_scaled = np.maximum(0, fc_output_quant_scaled).astype(np.int64)\n",
        "    # No change in scale for ReLU\n",
        "\n",
        "    # --- Dequantize Fixed-Point Outputs for Comparison ---\n",
        "    fc_output_fixed_dequant = dequantize_value(fc_output_quant_scaled, FRAC_BITS)\n",
        "    relu1_output_fixed_dequant = dequantize_value(relu1_output_quant_scaled, FRAC_BITS)\n",
        "\n",
        "    print(f\"\\n--- Simulated Fixed-Point Outputs (Dequantized) ---\")\n",
        "    print(f\"FC output fixed (dequant) min: {fc_output_fixed_dequant.min():.4f}, max: {fc_output_fixed_dequant.max():.4f}\")\n",
        "    print(f\"ReLU1 output fixed (dequant) min: {relu1_output_fixed_dequant.min():.4f}, max: {relu1_output_fixed_dequant.max():.4f}\")\n",
        "\n",
        "    # --- Compare Outputs ---\n",
        "    mse_fc = np.mean((fc_output_float_np - fc_output_fixed_dequant)**2)\n",
        "    mae_fc = np.mean(np.abs(fc_output_float_np - fc_output_fixed_dequant))\n",
        "    mse_relu1 = np.mean((relu1_output_float_np - relu1_output_fixed_dequant)**2)\n",
        "    mae_relu1 = np.mean(np.abs(relu1_output_float_np - relu1_output_fixed_dequant))\n",
        "\n",
        "    print(f\"\\n--- Comparison of Layer Outputs (Float vs Fixed-Point Sim) ---\")\n",
        "    print(f\"FC Layer MSE: {mse_fc:.2e}, MAE: {mae_fc:.2e}\")\n",
        "    print(f\"ReLU1 Layer MSE: {mse_relu1:.2e}, MAE: {mae_relu1:.2e}\")\n",
        "\n",
        "    print(\"\\nNext steps: Perform thorough range analysis on your TRAINED model, then extend this simulation to Conv1D and Upsample layers.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L72KkLRnJoKh",
        "outputId": "adbe45db-f41e-4b61-9b15-a58c8e0f7ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Fixed-Point Conversion Analysis for ASIC PoC ---\n",
            "Activation fixed-point: TOTAL_BITS=16, FRAC_BITS=8\n",
            "Weight fixed-point: TOTAL_BITS=16, FRAC_BITS=10\n",
            "\n",
            "NOTE: Ensure you load your TRAINED model weights for accurate analysis.\n",
            "\n",
            "--- Range Analysis Methodology (Action Required) ---\n",
            "1. Generate a diverse set of N (e.g., 1000-10000) random latent vectors 'z'.\n",
            "2. For each 'z', perform a full forward pass through your TRAINED FLOATING-POINT decoder.\n",
            "3. Record the MIN and MAX floating-point values observed for:\n",
            "   - All weight tensors (e.g., decoder.fc.weight, decoder.conv1.weight, etc.)\n",
            "   - All bias tensors.\n",
            "   - The INPUT to EACH layer (activations).\n",
            "   - The OUTPUT of EACH layer (activations).\n",
            "4. Use these global MIN/MAX ranges to choose appropriate TOTAL_BITS and FRAC_BITS for weights and activations.\n",
            "   (You might use different bit-widths for weights vs. activations, or even per layer).\n",
            "-----------------------------------------------------\n",
            "\n",
            "FC Layer: fc_weights_quant shape: (8192, 64), dtype: int64\n",
            "FC Layer: fc_bias_quant shape: (8192,), dtype: int64\n",
            "\n",
            "--- Reference Floating-Point Outputs ---\n",
            "FC output float min: -1.8687, max: 1.8829\n",
            "ReLU1 output float min: 0.0000, max: 1.8829\n",
            "\n",
            "--- Simulated Fixed-Point Outputs (Dequantized) ---\n",
            "FC output fixed (dequant) min: -1.8672, max: 1.8867\n",
            "ReLU1 output fixed (dequant) min: 0.0000, max: 1.8867\n",
            "\n",
            "--- Comparison of Layer Outputs (Float vs Fixed-Point Sim) ---\n",
            "FC Layer MSE: 7.94e-06, MAE: 2.26e-03\n",
            "ReLU1 Layer MSE: 3.89e-06, MAE: 1.11e-03\n",
            "\n",
            "Next steps: Perform thorough range analysis on your TRAINED model, then extend this simulation to Conv1D and Upsample layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Code to Save Decoder Weights ---\n",
        "# Make sure your 'vae' model instance is trained and available in this scope.\n",
        "# For example, if your trained VAE model instance is named `vae`:\n",
        "\n",
        "# Define the path where you want to save the decoder's weights\n",
        "# You might want to save this in your Google Drive if using Colab, or a local path.\n",
        "# Example for Colab, assuming Drive is mounted at /content/drive:\n",
        "# output_directory = \"/content/drive/MyDrive/Neural_Drum_Machine_Output\"\n",
        "# Or a local path:\n",
        "output_directory = \"./model_weights\" # Saves in a 'model_weights' subdirectory\n",
        "os.makedirs(output_directory, exist_ok=True) # Create directory if it doesn't exist\n",
        "\n",
        "decoder_weights_filename = \"trained_simple_decoder_weights.pth\"\n",
        "decoder_weights_path = os.path.join(output_directory, decoder_weights_filename)\n",
        "\n",
        "# Check if the 'vae' model exists (it should after your training script runs)\n",
        "if 'vae' in locals() and isinstance(vae, SimpleVAE):\n",
        "    print(f\"Found trained VAE model instance 'vae'.\")\n",
        "\n",
        "    # Ensure the model is on the CPU before saving, for broader compatibility,\n",
        "    # unless you specifically need to save it with GPU information (not typical for state_dict).\n",
        "    vae.cpu() # Move the whole VAE to CPU; this moves its submodules (like decoder) too.\n",
        "\n",
        "    # Access the decoder submodule and save its state_dict\n",
        "    try:\n",
        "        torch.save(vae.decoder.state_dict(), decoder_weights_path)\n",
        "        print(f\"Successfully saved SimpleDecoder state_dict to: {decoder_weights_path}\")\n",
        "        print(\"\\nYou can now use this path in your fixed-point analysis script to load these weights.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving decoder weights: {e}\")\n",
        "else:\n",
        "    print(\"Error: Trained VAE model instance named 'vae' not found in the current scope.\")\n",
        "    print(\"Please ensure your VAE model is trained and the instance is named 'vae',\")\n",
        "    print(\"or modify this script to use the correct variable name for your trained VAE model.\")\n",
        "\n",
        "# To load these weights into a standalone SimpleDecoder instance later:\n",
        "#\n",
        "# device_for_loading = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# # 1. Instantiate a new SimpleDecoder (make sure class definition is available)\n",
        "# standalone_decoder = SimpleDecoder(latent_dim=LATENT_DIM, n_output_channels=N_MELS)\n",
        "# # 2. Load the saved state_dict\n",
        "# standalone_decoder.load_state_dict(torch.load(decoder_weights_path, map_location=device_for_loading))\n",
        "# # 3. Set to evaluation mode if you're doing inference\n",
        "# standalone_decoder.eval()\n",
        "# print(f\"\\nExample loading: standalone_decoder.load_state_dict(torch.load('{decoder_weights_path}'))\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IevYVE9XMA7o",
        "outputId": "4f775f82-7b14-4590-d568-719d9c3c8a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found trained VAE model instance 'vae'.\n",
            "Successfully saved SimpleDecoder state_dict to: ./model_weights/trained_simple_decoder_weights.pth\n",
            "\n",
            "You can now use this path in your fixed-point analysis script to load these weights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Constants (ensure these match your trained model) ---\n",
        "LATENT_DIM = 64\n",
        "N_MELS = 128\n",
        "FIXED_FRAMES = 512\n",
        "\n",
        "# --- SimpleDecoder Definition (Revised for detailed layer access) ---\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM, n_output_channels=N_MELS, fixed_frames=FIXED_FRAMES):\n",
        "        super().__init__()\n",
        "        self.n_output_channels = n_output_channels\n",
        "        self.initial_frames = fixed_frames // 4\n",
        "        self.initial_channels = 64\n",
        "\n",
        "        self.fc = nn.Linear(latent_dim, self.initial_channels * self.initial_frames)\n",
        "\n",
        "        # Manually define layers to match the structure from which weights were saved\n",
        "        # (e.g., if saved from an nn.Sequential block named 'decode_layers')\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv1 = nn.Conv1d(self.initial_channels, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv2 = nn.Conv1d(32, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.conv3 = nn.Conv1d(16, self.n_output_channels, kernel_size=1) # Final Mel conv\n",
        "        self.relu4 = nn.ReLU() # Final ReLU\n",
        "\n",
        "    def forward(self, z, analyse_all_layers=True): # Default to analyse_all_layers=True now\n",
        "        activations = {}\n",
        "\n",
        "        x = self.fc(z)\n",
        "        if analyse_all_layers: activations['fc_out'] = x.detach().clone()\n",
        "\n",
        "        x = self.relu1(x)\n",
        "        if analyse_all_layers: activations['relu1_out'] = x.detach().clone()\n",
        "\n",
        "        x = x.view(-1, self.initial_channels, self.initial_frames)\n",
        "        if analyse_all_layers: activations['reshape_out'] = x.detach().clone()\n",
        "\n",
        "        x = self.upsample1(x)\n",
        "        if analyse_all_layers: activations['upsample1_out'] = x.detach().clone()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        if analyse_all_layers: activations['conv1_out'] = x.detach().clone()\n",
        "\n",
        "        x = self.relu2(x)\n",
        "        if analyse_all_layers: activations['relu2_out'] = x.detach().clone()\n",
        "\n",
        "        x = self.upsample2(x)\n",
        "        if analyse_all_layers: activations['upsample2_out'] = x.detach().clone()\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        if analyse_all_layers: activations['conv2_out'] = x.detach().clone()\n",
        "\n",
        "        x = self.relu3(x)\n",
        "        if analyse_all_layers: activations['relu3_out'] = x.detach().clone()\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        if analyse_all_layers: activations['conv3_out_final_mel'] = x.detach().clone()\n",
        "\n",
        "        x = self.relu4(x)\n",
        "        if analyse_all_layers: activations['relu4_out_final'] = x.detach().clone()\n",
        "\n",
        "        final_output = x\n",
        "        if analyse_all_layers: return final_output, activations\n",
        "        else: return final_output # Just return final if not analysing all\n",
        "\n",
        "    def load_custom_state_dict(self, state_dict_path, device):\n",
        "        \"\"\"Loads state_dict where sequential layers were saved with 'decode_layers.X' prefix.\"\"\"\n",
        "        state_dict = torch.load(state_dict_path, map_location=device)\n",
        "        new_state_dict = self.state_dict()\n",
        "\n",
        "        # Manually map keys from 'decode_layers.X.param' to direct attribute names\n",
        "        key_map = {\n",
        "            # ReLU0 (self.relu1) has no params\n",
        "            # Upsample1 (self.upsample1) has no params\n",
        "            'decode_layers.2.weight': 'conv1.weight', 'decode_layers.2.bias': 'conv1.bias',\n",
        "            # ReLU1 (self.relu2) has no params\n",
        "            # Upsample2 (self.upsample2) has no params\n",
        "            'decode_layers.5.weight': 'conv2.weight', 'decode_layers.5.bias': 'conv2.bias',\n",
        "            # ReLU2 (self.relu3) has no params\n",
        "            'decode_layers.7.weight': 'conv3.weight', 'decode_layers.7.bias': 'conv3.bias',\n",
        "            # ReLU3 (self.relu4) has no params\n",
        "        }\n",
        "\n",
        "        loaded_count = 0\n",
        "        # Load fc layer directly\n",
        "        if 'fc.weight' in state_dict: new_state_dict['fc.weight'] = state_dict['fc.weight']; loaded_count+=1\n",
        "        if 'fc.bias' in state_dict: new_state_dict['fc.bias'] = state_dict['fc.bias']; loaded_count+=1\n",
        "\n",
        "        for old_key_prefix_idx, new_attr_base in [('2','conv1'), ('5','conv2'), ('7','conv3')]:\n",
        "            old_w_key = f'decode_layers.{old_key_prefix_idx}.weight'\n",
        "            old_b_key = f'decode_layers.{old_key_prefix_idx}.bias'\n",
        "            new_w_key = f'{new_attr_base}.weight'\n",
        "            new_b_key = f'{new_attr_base}.bias'\n",
        "\n",
        "            if old_w_key in state_dict: new_state_dict[new_w_key] = state_dict[old_w_key]; loaded_count+=1\n",
        "            if old_b_key in state_dict: new_state_dict[new_b_key] = state_dict[old_b_key]; loaded_count+=1\n",
        "\n",
        "        try:\n",
        "            self.load_state_dict(new_state_dict)\n",
        "            print(f\"Successfully loaded and mapped weights ({loaded_count} tensors) from: {state_dict_path}\\n\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"RuntimeError during load_state_dict after mapping: {e}\")\n",
        "            print(\"Current model keys:\", self.state_dict().keys())\n",
        "            print(\"Attempted to load with keys:\", new_state_dict.keys())\n",
        "            print(\"Original loaded state_dict keys:\", state_dict.keys())\n",
        "            raise e\n",
        "\n",
        "# --- Chosen Fixed-Point Simulation Parameters (Based on Your Range Analysis) ---\n",
        "ACT_TOTAL_BITS = 16\n",
        "ACT_FRAC_BITS = 7\n",
        "WEIGHT_TOTAL_BITS = 16\n",
        "WEIGHT_FRAC_BITS = 13\n",
        "BIAS_TOTAL_BITS = ACT_TOTAL_BITS\n",
        "BIAS_FRAC_BITS = ACT_FRAC_BITS\n",
        "\n",
        "# --- Quantization and Dequantization Functions ---\n",
        "def quantize_value(value_np, total_bits, frac_bits):\n",
        "    scale = 2.0**frac_bits\n",
        "    min_val_representable = -(2.0**(total_bits - 1))\n",
        "    max_val_representable = (2.0**(total_bits - 1)) - 1\n",
        "    scaled_value = np.round(value_np * scale)\n",
        "    clamped_value = np.clip(scaled_value, min_val_representable, max_val_representable)\n",
        "    return clamped_value.astype(np.int64)\n",
        "\n",
        "def dequantize_value(quantized_value_scaled, frac_bits):\n",
        "    scale = 2.0**frac_bits\n",
        "    return quantized_value_scaled / scale\n",
        "\n",
        "# --- Conv1D Fixed-Point Simulation Function ---\n",
        "def simulate_conv1d_fixed_point(input_quant_scaled, conv_layer_torch,\n",
        "                                weights_quant, bias_quant,\n",
        "                                act_total_bits, act_frac_bits,\n",
        "                                weight_frac_bits, bias_frac_bits): # Added bias_frac_bits\n",
        "\n",
        "    in_channels, in_length = input_quant_scaled.shape\n",
        "    out_channels = conv_layer_torch.out_channels\n",
        "    kernel_size = conv_layer_torch.kernel_size[0]\n",
        "    padding = conv_layer_torch.padding[0]\n",
        "    # stride = conv_layer_torch.stride[0] # Assuming stride 1 for this sim\n",
        "\n",
        "    # Output length for Conv1D with padding P, kernel K, stride S=1: L_out = L_in - K + 2P + 1 (Incorrect formula)\n",
        "    # L_out = floor((L_in + 2P - K) / S) + 1. For S=1, L_out = L_in + 2P - K + 1\n",
        "    # If padding = 'same' effectively (K=3, P=1 -> L_out = L_in)\n",
        "    out_length = in_length # Assuming 'same' padding effect\n",
        "\n",
        "    padded_input_quant = np.pad(\n",
        "        input_quant_scaled,\n",
        "        ((0,0), (padding, padding)),\n",
        "        mode='constant',\n",
        "        constant_values=0 # Quantized zero\n",
        "    )\n",
        "\n",
        "    output_quant_scaled = np.zeros((out_channels, out_length), dtype=np.int64)\n",
        "    # Factor to rescale product (act_q * weight_q) before adding bias_q\n",
        "    product_rescale_factor = 2.0**weight_frac_bits\n",
        "\n",
        "    for oc in range(out_channels):\n",
        "        for l_out in range(out_length):\n",
        "            accumulator = np.int64(0)\n",
        "            for ic in range(in_channels):\n",
        "                for k_idx in range(kernel_size):\n",
        "                    input_val_q = padded_input_quant[ic, l_out + k_idx] # Stride 1 assumed\n",
        "                    weight_val_q = weights_quant[oc, ic, k_idx]\n",
        "                    prod = np.int64(input_val_q) * np.int64(weight_val_q)\n",
        "                    accumulator += prod\n",
        "\n",
        "            acc_rescaled = np.round(accumulator / product_rescale_factor).astype(np.int64)\n",
        "            # Bias is already quantized to its own Q format (here, same as activation)\n",
        "            output_quant_scaled[oc, l_out] = acc_rescaled + bias_quant[oc]\n",
        "\n",
        "    output_quant_scaled = np.clip(output_quant_scaled, -(2**(act_total_bits-1)), (2**(act_total_bits-1))-1)\n",
        "    return output_quant_scaled\n",
        "\n",
        "# --- Main Fixed-Point Analysis Script ---\n",
        "if __name__ == '__main__':\n",
        "    print(f\"--- Full Decoder Fixed-Point Simulation ---\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "    decoder = SimpleDecoder(latent_dim=LATENT_DIM, n_output_channels=N_MELS, fixed_frames=FIXED_FRAMES)\n",
        "    decoder.to(device)\n",
        "\n",
        "    decoder_weights_path = \"./model_weights/trained_simple_decoder_weights.pth\"\n",
        "    if os.path.exists(decoder_weights_path):\n",
        "        decoder.load_custom_state_dict(decoder_weights_path, device)\n",
        "    else:\n",
        "        print(f\"Trained decoder weights not found at {decoder_weights_path}. Exiting.\")\n",
        "        exit()\n",
        "    decoder.eval()\n",
        "\n",
        "    print(f\"\\n--- Using Chosen Fixed-Point Simulation Parameters ---\")\n",
        "    act_int_bits = ACT_TOTAL_BITS - ACT_FRAC_BITS - 1\n",
        "    weight_int_bits = WEIGHT_TOTAL_BITS - WEIGHT_FRAC_BITS - 1\n",
        "    bias_int_bits = BIAS_TOTAL_BITS - BIAS_FRAC_BITS - 1\n",
        "    print(f\"Activation Q-format: Q{act_int_bits}.{ACT_FRAC_BITS} (Total: {ACT_TOTAL_BITS})\")\n",
        "    print(f\"Weight Q-format: Q{weight_int_bits}.{WEIGHT_FRAC_BITS} (Total: {WEIGHT_TOTAL_BITS})\")\n",
        "    print(f\"Bias Q-format: Q{bias_int_bits}.{BIAS_FRAC_BITS} (Total: {BIAS_TOTAL_BITS})\")\n",
        "\n",
        "    # --- Quantize all necessary weights and biases ---\n",
        "    fc_weights_q = quantize_value(decoder.fc.weight.data.cpu().numpy(), WEIGHT_TOTAL_BITS, WEIGHT_FRAC_BITS)\n",
        "    fc_bias_q = quantize_value(decoder.fc.bias.data.cpu().numpy(), BIAS_TOTAL_BITS, BIAS_FRAC_BITS)\n",
        "\n",
        "    conv1_weights_q = quantize_value(decoder.conv1.weight.data.cpu().numpy(), WEIGHT_TOTAL_BITS, WEIGHT_FRAC_BITS)\n",
        "    conv1_bias_q = quantize_value(decoder.conv1.bias.data.cpu().numpy(), BIAS_TOTAL_BITS, BIAS_FRAC_BITS)\n",
        "\n",
        "    conv2_weights_q = quantize_value(decoder.conv2.weight.data.cpu().numpy(), WEIGHT_TOTAL_BITS, WEIGHT_FRAC_BITS)\n",
        "    conv2_bias_q = quantize_value(decoder.conv2.bias.data.cpu().numpy(), BIAS_TOTAL_BITS, BIAS_FRAC_BITS)\n",
        "\n",
        "    conv3_weights_q = quantize_value(decoder.conv3.weight.data.cpu().numpy(), WEIGHT_TOTAL_BITS, WEIGHT_FRAC_BITS)\n",
        "    conv3_bias_q = quantize_value(decoder.conv3.bias.data.cpu().numpy(), BIAS_TOTAL_BITS, BIAS_FRAC_BITS)\n",
        "\n",
        "    # --- Single Sample Simulation ---\n",
        "    sim_z_float_tensor = torch.randn(1, LATENT_DIM, device=device)\n",
        "\n",
        "    # --- Floating-Point Reference Pass (Full Decoder) ---\n",
        "    ref_final_output_float, ref_activations_float = decoder.forward(sim_z_float_tensor, analyse_all_layers=True)\n",
        "\n",
        "    # --- Fixed-Point Simulation (Full Decoder) ---\n",
        "    fixed_activations_quant_scaled = {} # To store quantized outputs of each layer\n",
        "    fixed_activations_dequant = {}    # To store dequantized outputs for comparison\n",
        "\n",
        "    # 1. Input Quantization\n",
        "    current_input_quant_scaled = quantize_value(sim_z_float_tensor.numpy().squeeze(), ACT_TOTAL_BITS, ACT_FRAC_BITS)\n",
        "    fixed_activations_quant_scaled['latent_z'] = current_input_quant_scaled\n",
        "    fixed_activations_dequant['latent_z'] = dequantize_value(current_input_quant_scaled, ACT_FRAC_BITS)\n",
        "\n",
        "    # 2. FC Layer\n",
        "    fc_output_q_s = np.zeros(decoder.fc.out_features, dtype=np.int64)\n",
        "    product_rescale_fc = 2.0**WEIGHT_FRAC_BITS\n",
        "    for i in range(decoder.fc.out_features):\n",
        "        acc = np.int64(0)\n",
        "        for j in range(decoder.fc.in_features):\n",
        "            acc += np.int64(current_input_quant_scaled[j]) * np.int64(fc_weights_q[i, j])\n",
        "        fc_output_q_s[i] = np.round(acc / product_rescale_fc).astype(np.int64) + fc_bias_q[i]\n",
        "    current_output_quant_scaled = np.clip(fc_output_q_s, -(2**(ACT_TOTAL_BITS-1)), (2**(ACT_TOTAL_BITS-1))-1)\n",
        "    fixed_activations_quant_scaled['fc_out'] = current_output_quant_scaled\n",
        "    fixed_activations_dequant['fc_out'] = dequantize_value(current_output_quant_scaled, ACT_FRAC_BITS)\n",
        "\n",
        "    # 3. ReLU1\n",
        "    current_output_quant_scaled = np.maximum(0, current_output_quant_scaled).astype(np.int64)\n",
        "    fixed_activations_quant_scaled['relu1_out'] = current_output_quant_scaled\n",
        "    fixed_activations_dequant['relu1_out'] = dequantize_value(current_output_quant_scaled, ACT_FRAC_BITS)\n",
        "\n",
        "    # 4. Reshape\n",
        "    current_output_quant_scaled = current_output_quant_scaled.reshape((decoder.initial_channels, decoder.initial_frames))\n",
        "    fixed_activations_quant_scaled['reshape_out'] = current_output_quant_scaled\n",
        "    fixed_activations_dequant['reshape_out'] = dequantize_value(current_output_quant_scaled.flatten(), ACT_FRAC_BITS).reshape(current_output_quant_scaled.shape)\n",
        "\n",
        "\n",
        "    # 5. Upsample1\n",
        "    current_output_quant_scaled = np.repeat(current_output_quant_scaled, 2, axis=1)\n",
        "    fixed_activations_quant_scaled['upsample1_out'] = current_output_quant_scaled\n",
        "    fixed_activations_dequant['upsample1_out'] = dequantize_value(current_output_quant_scaled.flatten(), ACT_FRAC_BITS).reshape(current_output_quant_scaled.shape)\n",
        "\n",
        "    # 6. Conv1\n",
        "    current_output_quant_scaled = simulate_conv1d_fixed_point(\n",
        "        current_output_quant_scaled, decoder.conv1, conv1_weights_q, conv1_bias_q,\n",
        "        ACT_TOTAL_BITS, ACT_FRAC_BITS, WEIGHT_FRAC_BITS, BIAS_FRAC_BITS\n",
        "    )\n",
        "    fixed_activations_quant_scaled['conv1_out'] = current_output_quant_scaled\n",
        "    fixed_activations_dequant['conv1_out'] = dequantize_value(current_output_quant_scaled.flatten(), ACT_FRAC_BITS).reshape(current_output_quant_scaled.shape)\n",
        "\n",
        "    # 7. ReLU2\n",
        "    current_output_quant_scaled = np.maximum(0, current_output_quant_scaled).astype(np.int64)\n",
        "    fixed_activations_quant_scaled['relu2_out'] = current_output_quant_scaled\n",
        "    fixed_activations_dequant['relu2_out'] = dequantize_value(current_output_quant_scaled.flatten(), ACT_FRAC_BITS).reshape(current_output_quant_scaled.shape)\n",
        "\n",
        "    # 8. Upsample2\n",
        "    current_output_quant_scaled = np.repeat(current_output_quant_scaled, 2, axis=1)\n",
        "    fixed_activations_quant_scaled['upsample2_out'] = current_output_quant_scaled\n",
        "    fixed_activations_dequant['upsample2_out'] = dequantize_value(current_output_quant_scaled.flatten(), ACT_FRAC_BITS).reshape(current_output_quant_scaled.shape)\n",
        "\n",
        "    # 9. Conv2\n",
        "    current_output_quant_scaled = simulate_conv1d_fixed_point(\n",
        "        current_output_quant_scaled, decoder.conv2, conv2_weights_q, conv2_bias_q,\n",
        "        ACT_TOTAL_BITS, ACT_FRAC_BITS, WEIGHT_FRAC_BITS, BIAS_FRAC_BITS\n",
        "    )\n",
        "    fixed_activations_quant_scaled['conv2_out'] = current_output_quant_scaled\n",
        "    fixed_activations_dequant['conv2_out'] = dequantize_value(current_output_quant_scaled.flatten(), ACT_FRAC_BITS).reshape(current_output_quant_scaled.shape)\n",
        "\n",
        "    # 10. ReLU3\n",
        "    current_output_quant_scaled = np.maximum(0, current_output_quant_scaled).astype(np.int64)\n",
        "    fixed_activations_quant_scaled['relu3_out'] = current_output_quant_scaled\n",
        "    fixed_activations_dequant['relu3_out'] = dequantize_value(current_output_quant_scaled.flatten(), ACT_FRAC_BITS).reshape(current_output_quant_scaled.shape)\n",
        "\n",
        "    # 11. Conv3 (Final Mel)\n",
        "    current_output_quant_scaled = simulate_conv1d_fixed_point(\n",
        "        current_output_quant_scaled, decoder.conv3, conv3_weights_q, conv3_bias_q,\n",
        "        ACT_TOTAL_BITS, ACT_FRAC_BITS, WEIGHT_FRAC_BITS, BIAS_FRAC_BITS\n",
        "    )\n",
        "    fixed_activations_quant_scaled['conv3_out_final_mel'] = current_output_quant_scaled\n",
        "    fixed_activations_dequant['conv3_out_final_mel'] = dequantize_value(current_output_quant_scaled.flatten(), ACT_FRAC_BITS).reshape(current_output_quant_scaled.shape)\n",
        "\n",
        "    # 12. ReLU4 (Final)\n",
        "    current_output_quant_scaled = np.maximum(0, current_output_quant_scaled).astype(np.int64)\n",
        "    fixed_activations_quant_scaled['relu4_out_final'] = current_output_quant_scaled\n",
        "    fixed_activations_dequant['relu4_out_final'] = dequantize_value(current_output_quant_scaled.flatten(), ACT_FRAC_BITS).reshape(current_output_quant_scaled.shape)\n",
        "\n",
        "    # --- Compare All Layer Outputs ---\n",
        "    print(f\"\\n--- Comparison of ALL Layer Outputs (using DERIVED Q-formats) ---\")\n",
        "    activation_keys_ordered = [ # Ensure order matches forward pass for clarity\n",
        "        'fc_out', 'relu1_out', 'reshape_out', 'upsample1_out',\n",
        "        'conv1_out', 'relu2_out', 'upsample2_out', 'conv2_out',\n",
        "        'relu3_out', 'conv3_out_final_mel', 'relu4_out_final'\n",
        "    ]\n",
        "\n",
        "    for key in activation_keys_ordered:\n",
        "        if key in ref_activations_float and key in fixed_activations_dequant:\n",
        "            ref_val_np = ref_activations_float[key].cpu().numpy().squeeze()\n",
        "            fixed_val_dequant_np = fixed_activations_dequant[key] # Already squeezed if 1D, or reshaped\n",
        "\n",
        "            # Ensure shapes match for comparison, especially after squeeze\n",
        "            if ref_val_np.shape != fixed_val_dequant_np.shape:\n",
        "                # print(f\"Shape mismatch for {key}: Ref {ref_val_np.shape}, Fixed {fixed_val_dequant_np.shape}. Skipping comparison for this key.\")\n",
        "                # Attempt to reshape fixed if it was flattened during dequant for some reason\n",
        "                try:\n",
        "                    fixed_val_dequant_np = fixed_val_dequant_np.reshape(ref_val_np.shape)\n",
        "                except ValueError:\n",
        "                    print(f\"Could not reshape fixed_val_dequant_np for key {key} to match reference. Ref shape: {ref_val_np.shape}, Fixed shape: {fixed_val_dequant_np.shape}\")\n",
        "                    continue\n",
        "\n",
        "\n",
        "            mse = np.mean((ref_val_np - fixed_val_dequant_np)**2)\n",
        "            mae = np.mean(np.abs(ref_val_np - fixed_val_dequant_np))\n",
        "            print(f\"{key} -> MSE: {mse:.3e}, MAE: {mae:.3e}\")\n",
        "        else:\n",
        "            print(f\"Key {key} not found in reference or fixed-point activations for comparison.\")\n",
        "\n",
        "    print(\"\\nExamine the MSE/MAE for 'relu4_out_final' (the final output).\")\n",
        "    print(\"If this final error is acceptable, your Q-formats are good for the PoC.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPHd5cnGNJ_k",
        "outputId": "ef69a331-363a-436b-cf0a-c85cb8c7eed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Full Decoder Fixed-Point Simulation ---\n",
            "Successfully loaded and mapped weights (8 tensors) from: ./model_weights/trained_simple_decoder_weights.pth\n",
            "\n",
            "\n",
            "--- Using Chosen Fixed-Point Simulation Parameters ---\n",
            "Activation Q-format: Q8.7 (Total: 16)\n",
            "Weight Q-format: Q2.13 (Total: 16)\n",
            "Bias Q-format: Q8.7 (Total: 16)\n",
            "\n",
            "--- Comparison of ALL Layer Outputs (using DERIVED Q-formats) ---\n",
            "fc_out -> MSE: 2.016e-05, MAE: 3.557e-03\n",
            "relu1_out -> MSE: 7.458e-06, MAE: 1.278e-03\n",
            "reshape_out -> MSE: 7.458e-06, MAE: 1.278e-03\n",
            "upsample1_out -> MSE: 7.458e-06, MAE: 1.278e-03\n",
            "conv1_out -> MSE: 1.031e-04, MAE: 7.336e-03\n",
            "relu2_out -> MSE: 1.952e-05, MAE: 1.307e-03\n",
            "upsample2_out -> MSE: 1.952e-05, MAE: 1.307e-03\n",
            "conv2_out -> MSE: 1.335e-04, MAE: 7.789e-03\n",
            "relu3_out -> MSE: 7.558e-05, MAE: 3.287e-03\n",
            "conv3_out_final_mel -> MSE: 6.174e-05, MAE: 5.369e-03\n",
            "relu4_out_final -> MSE: 4.727e-08, MAE: 1.151e-05\n",
            "\n",
            "Examine the MSE/MAE for 'relu4_out_final' (the final output).\n",
            "If this final error is acceptable, your Q-formats are good for the PoC.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Constants (ensure these match your trained model) ---\n",
        "LATENT_DIM = 64\n",
        "N_MELS = 128\n",
        "FIXED_FRAMES = 512\n",
        "\n",
        "# --- SimpleDecoder Definition (Corrected for state_dict loading) ---\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM, n_output_channels=N_MELS, fixed_frames=FIXED_FRAMES):\n",
        "        super().__init__()\n",
        "        self.n_output_channels = n_output_channels\n",
        "        self.initial_frames = fixed_frames // 4\n",
        "        self.initial_channels = 64\n",
        "\n",
        "        self.fc_out_features = self.initial_channels * self.initial_frames\n",
        "        self.fc = nn.Linear(latent_dim, self.fc_out_features)\n",
        "\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv1 = nn.Conv1d(self.initial_channels, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv2 = nn.Conv1d(32, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.conv3 = nn.Conv1d(16, self.n_output_channels, kernel_size=1) # Final Mel conv\n",
        "        self.relu4 = nn.ReLU() # Final ReLU\n",
        "\n",
        "    def forward(self, z, analyse_all_layers=True): # Default to analyse_all_layers=True now\n",
        "        activations = {}\n",
        "        x_current = self.fc(z)\n",
        "        if analyse_all_layers: activations['fc_out'] = x_current.detach().clone()\n",
        "        x_current = self.relu1(x_current)\n",
        "        if analyse_all_layers: activations['relu1_out'] = x_current.detach().clone()\n",
        "        x_current = x_current.view(-1, self.initial_channels, self.initial_frames)\n",
        "        if analyse_all_layers: activations['reshape_out'] = x_current.detach().clone()\n",
        "        x_current = self.upsample1(x_current)\n",
        "        if analyse_all_layers: activations['upsample1_out'] = x_current.detach().clone()\n",
        "        x_current = self.conv1(x_current)\n",
        "        if analyse_all_layers: activations['conv1_out'] = x_current.detach().clone()\n",
        "        x_current = self.relu2(x_current)\n",
        "        if analyse_all_layers: activations['relu2_out'] = x_current.detach().clone()\n",
        "        x_current = self.upsample2(x_current)\n",
        "        if analyse_all_layers: activations['upsample2_out'] = x_current.detach().clone()\n",
        "        x_current = self.conv2(x_current)\n",
        "        if analyse_all_layers: activations['conv2_out'] = x_current.detach().clone()\n",
        "        x_current = self.relu3(x_current)\n",
        "        if analyse_all_layers: activations['relu3_out'] = x_current.detach().clone()\n",
        "        x_current = self.conv3(x_current)\n",
        "        if analyse_all_layers: activations['conv3_out_final_mel'] = x_current.detach().clone()\n",
        "        x_current = self.relu4(x_current)\n",
        "        if analyse_all_layers: activations['relu4_out_final'] = x_current.detach().clone()\n",
        "        final_output = x_current\n",
        "        if analyse_all_layers: return final_output, activations\n",
        "        else: return final_output\n",
        "\n",
        "    def load_custom_state_dict(self, state_dict_path, device):\n",
        "        state_dict = torch.load(state_dict_path, map_location=device)\n",
        "        new_state_dict = self.state_dict()\n",
        "        loaded_count = 0\n",
        "        if 'fc.weight' in state_dict: new_state_dict['fc.weight'] = state_dict['fc.weight']; loaded_count+=1\n",
        "        if 'fc.bias' in state_dict: new_state_dict['fc.bias'] = state_dict['fc.bias']; loaded_count+=1\n",
        "        for old_key_prefix_idx, new_attr_base in [('2','conv1'), ('5','conv2'), ('7','conv3')]:\n",
        "            old_w_key = f'decode_layers.{old_key_prefix_idx}.weight'\n",
        "            old_b_key = f'decode_layers.{old_key_prefix_idx}.bias'\n",
        "            new_w_key = f'{new_attr_base}.weight'\n",
        "            new_b_key = f'{new_attr_base}.bias'\n",
        "            if old_w_key in state_dict: new_state_dict[new_w_key] = state_dict[old_w_key]; loaded_count+=1\n",
        "            if old_b_key in state_dict: new_state_dict[new_b_key] = state_dict[old_b_key]; loaded_count+=1\n",
        "        try:\n",
        "            self.load_state_dict(new_state_dict)\n",
        "            print(f\"Successfully loaded and mapped weights ({loaded_count} unique tensors mapped) from: {state_dict_path}\\n\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"RuntimeError during load_state_dict after mapping: {e}\")\n",
        "            raise e\n",
        "\n",
        "# --- Chosen Fixed-Point Simulation Parameters (Based on Your Range Analysis) ---\n",
        "ACT_TOTAL_BITS = 16\n",
        "ACT_FRAC_BITS = 7\n",
        "WEIGHT_TOTAL_BITS = 16\n",
        "WEIGHT_FRAC_BITS = 13\n",
        "BIAS_TOTAL_BITS = ACT_TOTAL_BITS\n",
        "BIAS_FRAC_BITS = ACT_FRAC_BITS\n",
        "\n",
        "# --- Quantization and Dequantization Functions ---\n",
        "def quantize_value(value_np, total_bits, frac_bits):\n",
        "    scale = 2.0**frac_bits\n",
        "    min_val_representable = -(2.0**(total_bits - 1))\n",
        "    max_val_representable = (2.0**(total_bits - 1)) - 1\n",
        "    scaled_value = np.round(value_np * scale)\n",
        "    clamped_value = np.clip(scaled_value, min_val_representable, max_val_representable)\n",
        "    return clamped_value.astype(np.int64)\n",
        "\n",
        "# ... (dequantize_value and MinMaxTracker are the same, can be omitted for brevity if already defined)\n",
        "def dequantize_value(quantized_value_scaled, frac_bits):\n",
        "    scale = 2.0**frac_bits\n",
        "    return quantized_value_scaled / scale\n",
        "\n",
        "# --- Main Fixed-Point Analysis Script ---\n",
        "if __name__ == '__main__':\n",
        "    print(f\"--- Generating Test Vectors for DotProductBiasUnit ---\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "    decoder = SimpleDecoder(latent_dim=LATENT_DIM, n_output_channels=N_MELS, fixed_frames=FIXED_FRAMES)\n",
        "    decoder.to(device)\n",
        "\n",
        "    decoder_weights_path = \"./model_weights/trained_simple_decoder_weights.pth\"\n",
        "    if os.path.exists(decoder_weights_path):\n",
        "        decoder.load_custom_state_dict(decoder_weights_path, device)\n",
        "    else:\n",
        "        print(f\"Trained decoder weights not found at {decoder_weights_path}. CANNOT GENERATE ACCURATE TEST VECTORS.\")\n",
        "        exit()\n",
        "    decoder.eval()\n",
        "\n",
        "    # --- Quantize FC weights and biases (needed for test vector generation) ---\n",
        "    fc_weights_q_for_tv = quantize_value(decoder.fc.weight.data.cpu().numpy(), WEIGHT_TOTAL_BITS, WEIGHT_FRAC_BITS)\n",
        "    fc_bias_q_for_tv = quantize_value(decoder.fc.bias.data.cpu().numpy(), BIAS_TOTAL_BITS, BIAS_FRAC_BITS)\n",
        "\n",
        "    # --- Generate a single, deterministic input sample for test vector ---\n",
        "    # Use a fixed seed for reproducibility of the \"random\" input\n",
        "    torch.manual_seed(42) # For PyTorch's random number generator\n",
        "    np.random.seed(42)    # For NumPy's random number generator (if used for z elsewhere)\n",
        "\n",
        "    sim_z_float_tensor_tv = torch.randn(1, LATENT_DIM, device=device)\n",
        "    sim_z_float_np_tv = sim_z_float_tensor_tv.numpy().squeeze() # (LATENT_DIM,)\n",
        "\n",
        "    # Quantize the input sample\n",
        "    sim_z_quant_tv = quantize_value(sim_z_float_np_tv, ACT_TOTAL_BITS, ACT_FRAC_BITS)\n",
        "\n",
        "    # --- Simulate FC Layer (Fixed-Point) for the chosen neuron to get expected output ---\n",
        "    neuron_index_for_tv = 0 # Test the first output neuron of the FC layer\n",
        "\n",
        "    fc_output_quant_scaled_tv = np.zeros(decoder.fc.out_features, dtype=np.int64)\n",
        "    product_rescale_fc_tv = 2.0**WEIGHT_FRAC_BITS\n",
        "\n",
        "    # Simulate only the neuron_index_for_tv\n",
        "    accumulator_tv = np.int64(0)\n",
        "    for j in range(decoder.fc.in_features): # Dot product for the specific neuron\n",
        "        prod_tv = np.int64(sim_z_quant_tv[j]) * np.int64(fc_weights_q_for_tv[neuron_index_for_tv, j])\n",
        "        accumulator_tv += prod_tv\n",
        "\n",
        "    acc_rescaled_tv = np.round(accumulator_tv / product_rescale_fc_tv).astype(np.int64)\n",
        "    neuron_output_q_s_tv = acc_rescaled_tv + fc_bias_q_for_tv[neuron_index_for_tv]\n",
        "\n",
        "    # Clamp the output\n",
        "    expected_neuron_output_quant_scaled = np.clip(\n",
        "        neuron_output_q_s_tv,\n",
        "        -(2**(ACT_TOTAL_BITS - 1)),\n",
        "        (2**(ACT_TOTAL_BITS - 1)) - 1\n",
        "    )\n",
        "\n",
        "    # --- Print Test Vectors for Verilog ---\n",
        "    print(\"\\n--- Verilog Test Vectors for DotProductBiasUnit (neuron_index = 0) ---\")\n",
        "    print(f\"// Fixed-Point Formats used for generation:\")\n",
        "    print(f\"// Activation: Q{ACT_TOTAL_BITS-ACT_FRAC_BITS-1}.{ACT_FRAC_BITS} (Total: {ACT_TOTAL_BITS})\")\n",
        "    print(f\"// Weight:     Q{WEIGHT_TOTAL_BITS-WEIGHT_FRAC_BITS-1}.{WEIGHT_FRAC_BITS} (Total: {WEIGHT_TOTAL_BITS})\")\n",
        "    print(f\"// Bias:       Q{BIAS_TOTAL_BITS-BIAS_FRAC_BITS-1}.{BIAS_FRAC_BITS} (Total: {BIAS_TOTAL_BITS})\\n\")\n",
        "\n",
        "    print(f\"// Input Vector (sim_z_quant_tv) - {LATENT_DIM} elements, {ACT_TOTAL_BITS}-bit signed decimal:\")\n",
        "    for i in range(LATENT_DIM):\n",
        "        print(f\"input_vector_regs[{i}] = {ACT_TOTAL_BITS}'sd{sim_z_quant_tv[i]};\")\n",
        "\n",
        "    print(f\"\\n// Weight Vector for neuron {neuron_index_for_tv} (fc_weights_q_for_tv[{neuron_index_for_tv}, :]) - {LATENT_DIM} elements, {WEIGHT_TOTAL_BITS}-bit signed decimal:\")\n",
        "    for i in range(LATENT_DIM):\n",
        "        print(f\"weight_vector_regs[{i}] = {WEIGHT_TOTAL_BITS}'sd{fc_weights_q_for_tv[neuron_index_for_tv, i]};\")\n",
        "\n",
        "    print(f\"\\n// Bias Value for neuron {neuron_index_for_tv} (fc_bias_q_for_tv[{neuron_index_for_tv}]) - {BIAS_TOTAL_BITS}-bit signed decimal:\")\n",
        "    print(f\"bias_value_reg = {BIAS_TOTAL_BITS}'sd{fc_bias_q_for_tv[neuron_index_for_tv]};\")\n",
        "\n",
        "    print(f\"\\n// Expected Output Neuron Value (scaled integer) - {ACT_TOTAL_BITS}-bit signed decimal:\")\n",
        "    print(f\"// expected_output_neuron_scaled = {ACT_TOTAL_BITS}'sd{expected_neuron_output_quant_scaled};\")\n",
        "\n",
        "    # Also print dequantized version for human understanding\n",
        "    expected_neuron_output_dequant = dequantize_value(expected_neuron_output_quant_scaled, ACT_FRAC_BITS)\n",
        "    print(f\"// Expected Output Neuron Value (dequantized float): {expected_neuron_output_dequant:.6f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMmZHMxZN82R",
        "outputId": "17d15f01-32cd-4e0c-fa72-b3543fdc1257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating Test Vectors for DotProductBiasUnit ---\n",
            "Successfully loaded and mapped weights (8 unique tensors mapped) from: ./model_weights/trained_simple_decoder_weights.pth\n",
            "\n",
            "\n",
            "--- Verilog Test Vectors for DotProductBiasUnit (neuron_index = 0) ---\n",
            "// Fixed-Point Formats used for generation:\n",
            "// Activation: Q8.7 (Total: 16)\n",
            "// Weight:     Q2.13 (Total: 16)\n",
            "// Bias:       Q8.7 (Total: 16)\n",
            "\n",
            "// Input Vector (sim_z_quant_tv) - 64 elements, 16-bit signed decimal:\n",
            "input_vector_regs[0] = 16'sd247;\n",
            "input_vector_regs[1] = 16'sd190;\n",
            "input_vector_regs[2] = 16'sd115;\n",
            "input_vector_regs[3] = 16'sd-270;\n",
            "input_vector_regs[4] = 16'sd87;\n",
            "input_vector_regs[5] = 16'sd-158;\n",
            "input_vector_regs[6] = 16'sd-6;\n",
            "input_vector_regs[7] = 16'sd-205;\n",
            "input_vector_regs[8] = 16'sd-96;\n",
            "input_vector_regs[9] = 16'sd211;\n",
            "input_vector_regs[10] = 16'sd-50;\n",
            "input_vector_regs[11] = 16'sd-180;\n",
            "input_vector_regs[12] = 16'sd-93;\n",
            "input_vector_regs[13] = 16'sd-72;\n",
            "input_vector_regs[14] = 16'sd-98;\n",
            "input_vector_regs[15] = 16'sd98;\n",
            "input_vector_regs[16] = 16'sd210;\n",
            "input_vector_regs[17] = 16'sd-20;\n",
            "input_vector_regs[18] = 16'sd-64;\n",
            "input_vector_regs[19] = 16'sd56;\n",
            "input_vector_regs[20] = 16'sd-97;\n",
            "input_vector_regs[21] = 16'sd138;\n",
            "input_vector_regs[22] = 16'sd103;\n",
            "input_vector_regs[23] = 16'sd215;\n",
            "input_vector_regs[24] = 16'sd164;\n",
            "input_vector_regs[25] = 16'sd166;\n",
            "input_vector_regs[26] = 16'sd78;\n",
            "input_vector_regs[27] = 16'sd171;\n",
            "input_vector_regs[28] = 16'sd-30;\n",
            "input_vector_regs[29] = 16'sd5;\n",
            "input_vector_regs[30] = 16'sd-32;\n",
            "input_vector_regs[31] = 16'sd110;\n",
            "input_vector_regs[32] = 16'sd-177;\n",
            "input_vector_regs[33] = 16'sd-112;\n",
            "input_vector_regs[34] = 16'sd-29;\n",
            "input_vector_regs[35] = 16'sd220;\n",
            "input_vector_regs[36] = 16'sd41;\n",
            "input_vector_regs[37] = 16'sd-54;\n",
            "input_vector_regs[38] = 16'sd39;\n",
            "input_vector_regs[39] = 16'sd-99;\n",
            "input_vector_regs[40] = 16'sd-199;\n",
            "input_vector_regs[41] = 16'sd127;\n",
            "input_vector_regs[42] = 16'sd-113;\n",
            "input_vector_regs[43] = 16'sd-77;\n",
            "input_vector_regs[44] = 16'sd-163;\n",
            "input_vector_regs[45] = 16'sd272;\n",
            "input_vector_regs[46] = 16'sd-158;\n",
            "input_vector_regs[47] = 16'sd-62;\n",
            "input_vector_regs[48] = 16'sd-117;\n",
            "input_vector_regs[49] = 16'sd-84;\n",
            "input_vector_regs[50] = 16'sd10;\n",
            "input_vector_regs[51] = 16'sd67;\n",
            "input_vector_regs[52] = 16'sd-62;\n",
            "input_vector_regs[53] = 16'sd152;\n",
            "input_vector_regs[54] = 16'sd-104;\n",
            "input_vector_regs[55] = 16'sd-94;\n",
            "input_vector_regs[56] = 16'sd-180;\n",
            "input_vector_regs[57] = 16'sd5;\n",
            "input_vector_regs[58] = 16'sd-8;\n",
            "input_vector_regs[59] = 16'sd86;\n",
            "input_vector_regs[60] = 16'sd-13;\n",
            "input_vector_regs[61] = 16'sd236;\n",
            "input_vector_regs[62] = 16'sd-152;\n",
            "input_vector_regs[63] = 16'sd177;\n",
            "\n",
            "// Weight Vector for neuron 0 (fc_weights_q_for_tv[0, :]) - 64 elements, 16-bit signed decimal:\n",
            "weight_vector_regs[0] = 16'sd-33;\n",
            "weight_vector_regs[1] = 16'sd-3;\n",
            "weight_vector_regs[2] = 16'sd-45;\n",
            "weight_vector_regs[3] = 16'sd211;\n",
            "weight_vector_regs[4] = 16'sd-420;\n",
            "weight_vector_regs[5] = 16'sd-117;\n",
            "weight_vector_regs[6] = 16'sd-25;\n",
            "weight_vector_regs[7] = 16'sd-2685;\n",
            "weight_vector_regs[8] = 16'sd106;\n",
            "weight_vector_regs[9] = 16'sd-77;\n",
            "weight_vector_regs[10] = 16'sd-23;\n",
            "weight_vector_regs[11] = 16'sd115;\n",
            "weight_vector_regs[12] = 16'sd-142;\n",
            "weight_vector_regs[13] = 16'sd-78;\n",
            "weight_vector_regs[14] = 16'sd148;\n",
            "weight_vector_regs[15] = 16'sd-116;\n",
            "weight_vector_regs[16] = 16'sd-501;\n",
            "weight_vector_regs[17] = 16'sd132;\n",
            "weight_vector_regs[18] = 16'sd77;\n",
            "weight_vector_regs[19] = 16'sd-5938;\n",
            "weight_vector_regs[20] = 16'sd109;\n",
            "weight_vector_regs[21] = 16'sd1169;\n",
            "weight_vector_regs[22] = 16'sd-94;\n",
            "weight_vector_regs[23] = 16'sd-5;\n",
            "weight_vector_regs[24] = 16'sd301;\n",
            "weight_vector_regs[25] = 16'sd-142;\n",
            "weight_vector_regs[26] = 16'sd138;\n",
            "weight_vector_regs[27] = 16'sd-191;\n",
            "weight_vector_regs[28] = 16'sd82;\n",
            "weight_vector_regs[29] = 16'sd-45;\n",
            "weight_vector_regs[30] = 16'sd57;\n",
            "weight_vector_regs[31] = 16'sd136;\n",
            "weight_vector_regs[32] = 16'sd50;\n",
            "weight_vector_regs[33] = 16'sd44;\n",
            "weight_vector_regs[34] = 16'sd-577;\n",
            "weight_vector_regs[35] = 16'sd-1397;\n",
            "weight_vector_regs[36] = 16'sd-33;\n",
            "weight_vector_regs[37] = 16'sd-17;\n",
            "weight_vector_regs[38] = 16'sd-55;\n",
            "weight_vector_regs[39] = 16'sd-60;\n",
            "weight_vector_regs[40] = 16'sd-320;\n",
            "weight_vector_regs[41] = 16'sd-535;\n",
            "weight_vector_regs[42] = 16'sd37;\n",
            "weight_vector_regs[43] = 16'sd-1556;\n",
            "weight_vector_regs[44] = 16'sd-1128;\n",
            "weight_vector_regs[45] = 16'sd-37;\n",
            "weight_vector_regs[46] = 16'sd16496;\n",
            "weight_vector_regs[47] = 16'sd102;\n",
            "weight_vector_regs[48] = 16'sd-262;\n",
            "weight_vector_regs[49] = 16'sd-854;\n",
            "weight_vector_regs[50] = 16'sd94;\n",
            "weight_vector_regs[51] = 16'sd35;\n",
            "weight_vector_regs[52] = 16'sd-153;\n",
            "weight_vector_regs[53] = 16'sd121;\n",
            "weight_vector_regs[54] = 16'sd-85;\n",
            "weight_vector_regs[55] = 16'sd55;\n",
            "weight_vector_regs[56] = 16'sd273;\n",
            "weight_vector_regs[57] = 16'sd153;\n",
            "weight_vector_regs[58] = 16'sd115;\n",
            "weight_vector_regs[59] = 16'sd-275;\n",
            "weight_vector_regs[60] = 16'sd-18;\n",
            "weight_vector_regs[61] = 16'sd678;\n",
            "weight_vector_regs[62] = 16'sd25;\n",
            "weight_vector_regs[63] = 16'sd149;\n",
            "\n",
            "// Bias Value for neuron 0 (fc_bias_q_for_tv[0]) - 16-bit signed decimal:\n",
            "bias_value_reg = 16'sd-152;\n",
            "\n",
            "// Expected Output Neuron Value (scaled integer) - 16-bit signed decimal:\n",
            "// expected_output_neuron_scaled = 16'sd-428;\n",
            "// Expected Output Neuron Value (dequantized float): -3.343750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y81fI90SVU7y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}