{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQb9c9u7yatq"
      },
      "outputs": [],
      "source": [
        "# --- Imports ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import os, glob, random\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# --- Constants ---\n",
        "TARGET_SR = 44100\n",
        "N_FFT = 1024\n",
        "HOP_LENGTH = 256\n",
        "LATENT_DIM = 64\n",
        "FIXED_FRAMES = 512  # <-- key change here\n",
        "TOTAL_EPOCHS = 30000\n",
        "CHUNK_SIZE = 500\n",
        "NUM_CHUNKS = TOTAL_EPOCHS // CHUNK_SIZE\n",
        "KL_TARGET = 0.1\n",
        "KL_WARMUP = 2000\n",
        "\n",
        "# --- Dataset ---\n",
        "folder_path = '/content/drive/MyDrive/Neural Drum Machine/Samples/01. Bass Drum'\n",
        "files_list = glob.glob(os.path.join(folder_path, '*.wav'))\n",
        "print(f\"Found {len(files_list)} bass drum samples.\")\n",
        "\n",
        "def wav_to_spec(filename, sr=TARGET_SR, n_fft=N_FFT, hop_length=HOP_LENGTH, target_frames=FIXED_FRAMES):\n",
        "    y, _ = librosa.load(filename, sr=sr)\n",
        "    y, _ = librosa.effects.trim(y, top_db=30)\n",
        "    y = librosa.util.fix_length(y, size=sr)\n",
        "    S = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
        "    mag = np.abs(S)\n",
        "    mag = np.log1p(mag)\n",
        "    mag = mag / mag.max()\n",
        "\n",
        "    # Pad or truncate to fixed length\n",
        "    if mag.shape[1] < target_frames:\n",
        "        mag = np.pad(mag, ((0, 0), (0, target_frames - mag.shape[1])))\n",
        "    else:\n",
        "        mag = mag[:, :target_frames]\n",
        "\n",
        "    return mag  # (513, 512)\n",
        "\n",
        "SAMPLES = np.stack([wav_to_spec(f) for f in files_list])\n",
        "print(f\"SAMPLES shape: {SAMPLES.shape}\")  # (N, 513, 512)\n",
        "\n",
        "class DrumDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, specs):\n",
        "        self.specs = torch.tensor(specs, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.specs)\n",
        "    def __getitem__(self, idx): return self.specs[idx]\n",
        "\n",
        "loader = torch.utils.data.DataLoader(DrumDataset(SAMPLES), batch_size=16, shuffle=True)\n",
        "\n",
        "# --- VAE ---\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(513, 256, 4, stride=2, padding=1),  # → (B, 256, 256)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(256, 128, 4, stride=2, padding=1),  # → (B, 128, 128)\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1),                      # → (B, 128, 1)\n",
        "        )\n",
        "        self.mu = nn.Linear(128, latent_dim)\n",
        "        self.logvar = nn.Linear(128, latent_dim)\n",
        "\n",
        "    def forward(self, x):  # x: (B, 513, 512)\n",
        "        h = self.conv(x).squeeze(-1)  # (B, 128)\n",
        "        return self.mu(h), self.logvar(h)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512 * 128),  # → reshape to (B, 512, 128)\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose1d(512, 256, 4, stride=2, padding=1),  # → (B, 256, 256)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(256, 128, 4, stride=2, padding=1),  # → (B, 128, 512)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 513, 1),                                # → (B, 513, 512)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.fc(z).view(-1, 512, 128)\n",
        "        return self.deconv(x)\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(latent_dim)\n",
        "        self.decoder = Decoder(latent_dim)\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encoder(x)\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        z = mu + std * torch.randn_like(std)\n",
        "        recon = self.decoder(z)\n",
        "        return recon, mu, logvar\n",
        "\n",
        "vae = VAE().cuda()\n",
        "opt = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "def kl_loss(mu, logvar):\n",
        "    return -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())\n",
        "\n",
        "def spec_to_audio(log_mag, sr=TARGET_SR, n_fft=N_FFT, hop_length=HOP_LENGTH):\n",
        "    mag = np.expm1(log_mag * log_mag.max())  # undo log1p and scaling\n",
        "    return librosa.griffinlim(mag, hop_length=hop_length, n_fft=n_fft)\n",
        "\n",
        "# --- Training ---\n",
        "start_epoch = 0\n",
        "for chunk in range(NUM_CHUNKS):\n",
        "    print(f\"\\n--- Training chunk {chunk+1}/{NUM_CHUNKS} (Epochs {start_epoch+1} to {start_epoch+CHUNK_SIZE}) ---\")\n",
        "\n",
        "    for epoch in range(start_epoch, start_epoch + CHUNK_SIZE):\n",
        "        vae.train(); total = 0\n",
        "        kl_weight = min(KL_TARGET, KL_TARGET * epoch / KL_WARMUP)\n",
        "\n",
        "        for batch in loader:  # (B, 513, 512)\n",
        "            batch = batch.cuda()\n",
        "            opt.zero_grad()\n",
        "            recon, mu, logvar = vae(batch)\n",
        "            recon_loss = F.l1_loss(recon, batch, reduction='sum')\n",
        "            kl = kl_loss(mu, logvar)\n",
        "            loss = recon_loss + kl_weight * kl\n",
        "            loss.backward(); opt.step()\n",
        "            total += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1} | Loss: {total / len(SAMPLES):.4f} | KL Weight: {kl_weight:.4f}\")\n",
        "\n",
        "    # --- Preview ---\n",
        "    vae.eval()\n",
        "    with torch.no_grad():\n",
        "        batch = next(iter(loader)).cuda()\n",
        "        idx = random.randint(0, batch.size(0) - 1)\n",
        "        real = batch[idx:idx+1]\n",
        "        recon, _, _ = vae(real)\n",
        "        real_np = real.squeeze().cpu().numpy()\n",
        "        recon_np = recon.squeeze().cpu().numpy()\n",
        "\n",
        "        z = torch.randn(1, LATENT_DIM).cuda()\n",
        "        fake = vae.decoder(z).cpu().squeeze().numpy()\n",
        "\n",
        "    plt.figure(figsize=(15, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    librosa.display.specshow(real_np, sr=TARGET_SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='linear')\n",
        "    plt.title(\"Original\")\n",
        "    plt.subplot(1, 3, 2)\n",
        "    librosa.display.specshow(recon_np, sr=TARGET_SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='linear')\n",
        "    plt.title(\"Reconstructed\")\n",
        "    plt.subplot(1, 3, 3)\n",
        "    librosa.display.specshow(fake, sr=TARGET_SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='linear')\n",
        "    plt.title(\"Random Sample\")\n",
        "    plt.suptitle(f\"Chunk {chunk+1}/{NUM_CHUNKS} — Epoch {start_epoch+CHUNK_SIZE}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Original Kick\")\n",
        "    display(Audio(spec_to_audio(real_np), rate=TARGET_SR))\n",
        "    print(\"Reconstructed Kick\")\n",
        "    display(Audio(spec_to_audio(recon_np), rate=TARGET_SR))\n",
        "    print(\"Randomly Generated Kick\")\n",
        "    display(Audio(spec_to_audio(fake), rate=TARGET_SR))\n",
        "\n",
        "    start_epoch += CHUNK_SIZE\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DK_rRDdWybqD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}